{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import chardet\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from pysentimiento import create_analyzer\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFileProcessor:\n",
    "    \"\"\"\n",
    "    Class to read text files and process messages.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): The path to the text file.\n",
    "\n",
    "    Methods:\n",
    "        read_file() -> Optional[str]: Reads the text file and returns its content as a string.\n",
    "        process_messages(txt_file: str) -> pd.DataFrame: Processes the text file content and returns a DataFrame with messages.\n",
    "        filter_users_by_message_count(df: pd.DataFrame, min_message_count: int) -> pd.DataFrame: Filters users based on the minimum message count.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the TextFileProcessor class.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the text file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_unicode_escape(s: str) -> str:\n",
    "        \"\"\"\n",
    "        Decodes Unicode escape sequences in a string.\n",
    "\n",
    "        Args:\n",
    "            s (str): The string with Unicode escape sequences.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        s = s.encode('utf-8').decode('utf-8')\n",
    "        return codecs.decode(s, 'unicode_escape')\n",
    "\n",
    "    def read_file(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Reads the text file and returns its content as a string.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: The content of the text file as a string, or None if there is an error.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the file is not found.\n",
    "            IOError: If there is an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Suprimir solo el aviso especÃ­fico\n",
    "            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                content = content.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')\n",
    "                content = unicodedata.normalize('NFKC', content)\n",
    "                return content\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found. {e}\")\n",
    "            return None\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file. {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def process_messages(txt_file: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Processes the text file content and returns a DataFrame with messages.\n",
    "\n",
    "        Args:\n",
    "            txt_file (str): The content of the text file as a string.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with columns \"DateHour\", \"User\", and \"Message\".\n",
    "        \"\"\"\n",
    "        # Regex patterns\n",
    "        patterns = {\n",
    "            'pattern_1': r\"(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}:\\d{2}) (AM|PM) - ([^:]+): (.*)\",\n",
    "            'pattern_2': r\"\\[(\\d{2}\\.\\d{2}\\.\\d{2}), (\\d{2}:\\d{2}:\\d{2})\\] ([^:]+): (.*)\",\n",
    "            'pattern_3': r\"\\[(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}:\\d{2}:\\d{2}) (AM|PM)\\] ([^:]+): (.*)\",\n",
    "            'pattern_4': r\"(\\d{2}/\\d{2}/\\d{2}), (\\d{1,2}:\\d{2}) ([ap]\\. m\\.) - ([^:]+): (.*)\",\n",
    "            'pattern_5': r'\\[(\\d{1,2}/\\d{1,2}/\\d{2,4}),\\s(\\d{1,2}:\\d{2}:\\d{2})\\s([ap]\\.?\\s[m]\\.?)\\]\\s([^\\:]+):\\s(.+)'\n",
    "        }\n",
    "\n",
    "        matches = {}\n",
    "        for key, pattern in patterns.items():\n",
    "            matches[key] = re.findall(pattern, txt_file)\n",
    "\n",
    "        df_messages = pd.DataFrame()\n",
    "        \n",
    "        if matches['pattern_1']:\n",
    "            df_messages = pd.DataFrame(matches['pattern_1'], columns=[\"Date\", \"Hour\", \"AM/PM\", \"User\", \"Message\"])\n",
    "            df_messages['DateHour'] = pd.to_datetime(df_messages['Date'] + ' ' + df_messages['Hour'] + ' ' + df_messages['AM/PM'], format='%m/%d/%y %I:%M %p')\n",
    "            df_messages = df_messages[[\"DateHour\", \"User\", \"Message\"]]\n",
    "        elif matches['pattern_2']:\n",
    "            df_messages = pd.DataFrame(matches['pattern_2'], columns=[\"Date\", \"Hour\", \"User\", \"Message\"])\n",
    "            df_messages['DateHour'] = pd.to_datetime(df_messages['Date'] + ' ' + df_messages['Hour'], format='%d.%m.%y %H:%M:%S')\n",
    "            df_messages = df_messages[[\"DateHour\", \"User\", \"Message\"]]\n",
    "        elif matches['pattern_3']:\n",
    "            df_messages = pd.DataFrame(matches['pattern_3'], columns=[\"Date\", \"Hour\", \"AM/PM\", \"User\", \"Message\"])\n",
    "            df_messages['DateHour'] = pd.to_datetime(df_messages['Date'] + ' ' + df_messages['Hour'] + ' ' + df_messages['AM/PM'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            df_messages = df_messages[[\"DateHour\", \"User\", \"Message\"]]\n",
    "        elif matches['pattern_4']:\n",
    "            df_messages = pd.DataFrame(matches['pattern_4'], columns=[\"Date\", \"Hour\", \"AM/PM\", \"User\", \"Message\"])\n",
    "            df_messages['AM/PM'] = df_messages['AM/PM'].replace({'a. m.': 'AM', 'p. m.': 'PM'})\n",
    "            df_messages['DateHour'] = pd.to_datetime(df_messages['Date'] + ' ' + df_messages['Hour'] + ' ' + df_messages['AM/PM'], format='%d/%m/%y %I:%M %p')\n",
    "            df_messages = df_messages[[\"DateHour\", \"User\", \"Message\"]]\n",
    "        elif matches['pattern_5']:\n",
    "            df_messages = pd.DataFrame(matches['pattern_5'], columns=[\"Date\", \"Hour\", \"AM/PM\", \"User\", \"Message\"])\n",
    "            df_messages['AM/PM'] = df_messages['AM/PM'].replace({'a. m.': 'AM', 'p. m.': 'PM'})\n",
    "            df_messages['DateHour'] = pd.to_datetime(df_messages['Date'] + ' ' + df_messages['Hour'] + ' ' + df_messages['AM/PM'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            df_messages = df_messages[[\"DateHour\", \"User\", \"Message\"]]\n",
    "        else:\n",
    "            print(\"No matches found.\")\n",
    "            df_messages = pd.DataFrame(columns=[\"DateHour\", \"User\", \"Message\"])\n",
    "\n",
    "        return df_messages\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_users_by_message_count(df: pd.DataFrame, min_message_count: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filters users based on the minimum message count.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with columns 'DateHour', 'User', and 'Message'.\n",
    "            min_message_count (int): Minimum number of messages a user must have to be included in the resulting DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered DataFrame with only users that have at least min_message_count messages.\n",
    "        \"\"\"\n",
    "        df_copy = df.copy()\n",
    "        user_message_counts = df_copy['User'].value_counts()\n",
    "        users_to_keep = user_message_counts[user_message_counts >= min_message_count].index\n",
    "        filtered_df = df_copy[df_copy['User'].isin(users_to_keep)].reset_index(drop=True)\n",
    "        return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TextFileProcessor(r'C:\\Users\\Admin\\Documents\\whatsapp-analyser\\data\\Chats\\WhatsApp Chat - Netflix  - Iphone (Spa)\\_chat.txt')\n",
    "\n",
    "file_content = processor.read_file()\n",
    "\n",
    "if file_content:\n",
    "    df_messages = processor.process_messages(file_content)\n",
    "    filtered_df = processor.filter_users_by_message_count(df_messages, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52468 entries, 0 to 52467\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   DateHour  52468 non-null  datetime64[ns]\n",
      " 1   User      52468 non-null  object        \n",
      " 2   Message   52468 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    \"\"\"\n",
    "    Class for analyzing text using different analysis tasks.\n",
    "\n",
    "    Attributes:\n",
    "        analyzer: Instance of the analyzer created with the specified task and language.\n",
    "        task (str): The specific task of the analysis (e.g., 'sentiment', 'emotion', etc.).\n",
    "    \"\"\"\n",
    "    def __init__(self, task: str, lang: str = \"es\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the TextAnalyzer with a specific task and language.\n",
    "\n",
    "        Args:\n",
    "            task (str): The specific task of the analysis.\n",
    "            lang (str): The language of the analysis (default is \"es\" for Spanish).\n",
    "        \"\"\"\n",
    "        self.analyzer = create_analyzer(task=task, lang=lang)\n",
    "        self.task = task\n",
    "\n",
    "    def predict(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Performs the prediction on the provided text and returns the results.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary with the probabilities and the maximum result.\n",
    "        \"\"\"\n",
    "        result = self.analyzer.predict(text)\n",
    "        result_dict = result.probas\n",
    "        max_key = f\"MAX_{self.task.upper()}\"\n",
    "        result_dict[max_key] = result.output\n",
    "        return result_dict\n",
    "\n",
    "class CombinedAnalyzer:\n",
    "    \"\"\"\n",
    "    Class to combine sentiment and emotion analyses.\n",
    "\n",
    "    Attributes:\n",
    "        sentiment_analyzer: Instance of TextAnalyzer for sentiment analysis.\n",
    "        emotion_analyzer: Instance of TextAnalyzer for emotion analysis.\n",
    "        return_mode (str): Specifies which analysis results to return ('both', 'sentiment', 'emotion').\n",
    "    \"\"\"\n",
    "    def __init__(self, return_mode: str = 'both') -> None:\n",
    "        \"\"\"\n",
    "        Initializes the analyzers for sentiment and emotion.\n",
    "\n",
    "        Args:\n",
    "            return_mode (str): Specifies which analysis results to return ('both', 'sentiment', 'emotion').\n",
    "                               Default is 'both'.\n",
    "        \"\"\"\n",
    "        self.sentiment_analyzer = TextAnalyzer(task=\"sentiment\")\n",
    "        self.emotion_analyzer = TextAnalyzer(task=\"emotion\")\n",
    "        self.return_mode = return_mode\n",
    "\n",
    "    def combine_sentiments(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Combines the results of sentiment and emotion analyses based on the return_mode.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A combined dictionary with the results of the analyses.\n",
    "        \"\"\"\n",
    "        sentiment_dict = self.sentiment_analyzer.predict(text)\n",
    "        emotion_dict = self.emotion_analyzer.predict(text)\n",
    "        \n",
    "        if self.return_mode == 'both':\n",
    "            combined_dict = {**sentiment_dict, **emotion_dict}\n",
    "        elif self.return_mode == 'sentiment':\n",
    "            combined_dict = sentiment_dict\n",
    "        elif self.return_mode == 'emotion':\n",
    "            combined_dict = emotion_dict\n",
    "        else:\n",
    "            raise ValueError(\"Invalid return_mode. Choose 'both', 'sentiment', or 'emotion'.\")\n",
    "        \n",
    "        return combined_dict\n",
    "\n",
    "    def analyze_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyzes a DataFrame of texts, applying sentiment and/or emotion analysis to each text.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame containing the texts to analyze.\n",
    "            text_column (str): The name of the column containing the texts.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The original DataFrame with added columns for analysis results.\n",
    "        \"\"\"\n",
    "        df[text_column] = df[text_column].apply(preprocess_tweet, lang=\"es\")\n",
    "        analysis_results = df[text_column].apply(self.combine_sentiments)\n",
    "        results_df = pd.json_normalize(analysis_results)\n",
    "        return pd.concat([df, results_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\whatsapp-analyser\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "analyzer = CombinedAnalyzer(return_mode = \"sentiment\")\n",
    "sample_results_sentiments = analyzer.analyze_dataframe(filtered_df, \"Message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self, task: str, lang: str = \"es\") -> None:\n",
    "        self.analyzer = create_analyzer(task=task, lang=lang)\n",
    "        self.task = task\n",
    "\n",
    "    def predict(self, text: str) -> Dict[str, float]:\n",
    "        result = self.analyzer.predict(text)\n",
    "        result_dict = result.probas\n",
    "        max_key = f\"MAX_{self.task.upper()}\"\n",
    "        result_dict[max_key] = result.output\n",
    "        return result_dict\n",
    "\n",
    "class CombinedAnalyzer:\n",
    "    def __init__(self, return_mode: str = 'both') -> None:\n",
    "        self.sentiment_analyzer = TextAnalyzer(task=\"sentiment\")\n",
    "        self.emotion_analyzer = TextAnalyzer(task=\"emotion\")\n",
    "        self.return_mode = return_mode\n",
    "\n",
    "    def combine_sentiments(self, text: str) -> Dict[str, float]:\n",
    "        sentiment_dict = self.sentiment_analyzer.predict(text)\n",
    "        emotion_dict = self.emotion_analyzer.predict(text)\n",
    "        \n",
    "        if self.return_mode == 'both':\n",
    "            combined_dict = {**sentiment_dict, **emotion_dict}\n",
    "        elif self.return_mode == 'sentiment':\n",
    "            combined_dict = sentiment_dict\n",
    "        elif self.return_mode == 'emotion':\n",
    "            combined_dict = emotion_dict\n",
    "        else:\n",
    "            raise ValueError(\"Invalid return_mode. Choose 'both', 'sentiment', or 'emotion'.\")\n",
    "        \n",
    "        return combined_dict\n",
    "\n",
    "    def analyze_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "\n",
    "        preprocessed_texts = df[text_column].apply(lambda x: preprocess_tweet(x, lang=\"es\")).tolist()\n",
    "        analysis_results = [self.combine_sentiments(text) for text in preprocessed_texts]\n",
    "        results_df = pd.json_normalize(analysis_results)\n",
    "        return pd.concat([df, results_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\whatsapp-analyser\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "analyzer = CombinedAnalyzer(return_mode='both')\n",
    "analyzed_df = analyzer.analyze_dataframe(filtered_df, 'Message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   DateHour       100 non-null    datetime64[ns]\n",
      " 1   User           100 non-null    object        \n",
      " 2   Message        100 non-null    object        \n",
      " 3   NEG            100 non-null    float64       \n",
      " 4   NEU            100 non-null    float64       \n",
      " 5   POS            100 non-null    float64       \n",
      " 6   MAX_SENTIMENT  100 non-null    object        \n",
      " 7   others         100 non-null    float64       \n",
      " 8   joy            100 non-null    float64       \n",
      " 9   sadness        100 non-null    float64       \n",
      " 10  anger          100 non-null    float64       \n",
      " 11  surprise       100 non-null    float64       \n",
      " 12  disgust        100 non-null    float64       \n",
      " 13  fear           100 non-null    float64       \n",
      " 14  MAX_EMOTION    100 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(10), object(4)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "analyzed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
